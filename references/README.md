## References
- SELU : [Self Normailizing Neural Networks](https://arxiv.org/abs/1706.02515)
- GELU : [Gaussian Error Linear Units (GELUs)- Elu ](https://arxiv.org/abs/1606.08415)
- PRELU : [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
- Leaky ReLU : [Rectifier Nonlinearities Improve Neural Network Acoustic Models](http://robotics.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)
- Swish : [Swish: a Self-Gated Activation Function](https://www.semanticscholar.org/paper/Swish%3A-a-Self-Gated-Activation-Function-Ramachandran-Zoph/4f57f486adea0bf95c252620a4e8af39232ef8bc)
    
- [Activation Functions Blog](https://mlfromscratch.com/activation-functions-explained/#elu)
